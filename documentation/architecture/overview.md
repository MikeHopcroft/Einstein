# Einstein Overview

## Terminology
* **Contestant** - a person or other authenticated entity that submits `Candidates` and runs `Suites`.
* **Candidate** - an instance of a system under evaluation. While the system may be composed of multiple services and implemented in any technology, it is exposed to the `Benchmark` as a container that runs in the `Laboratory Environment`. In order to ensure reproducibility, it is important that the container's external dependencies maintain the same behavior over time. If container were to call an external service whose behavior could be modified by, say, training, the container could exhibit different behavior over time, and this would impact reproducibility. A best practice is to version all external dependencies and create a separate container for each version.
* **Suite** - a collection of `Test Cases` used to evaluate a `Candidate` with respect to a specific `Benchmark`. Note that the `Laboratory Environment` will typically contain multiple `Suites`, each intended for a different purpose.
* **Test Case** - a single stimulus-response pair. The `Benchmark` provides the stimulus to the `Candidate`, which generates a response. This response is compared with the expected response.
* **Run** - the results generated by a specific `Candidate`, processing cases from a specific `Suite`. All `Runs` are archived.
Ideally, each `Test Run` is reproducible, meaning that any future `Test Run` involving the same `Test Suite` and `Candidate System` should produce the same results. If the `Candidate System` depends on external systems or data that can change over time, it may be impossible to reproduce the results of an earlier `Test Run`.
* **Data**-
* **Laboratory** - 
* **Laboratory Environment** - the collection of services and data stores used to assess `Candidates`. The `Laboratory Environment` stores all data, code and assets needed to run `Suites` and retrieve and reproduce historical results.
* **Benchmark** - a system that runs `Tests` by supplying cases from a `Suite` to a `Candidate`. The `Benchmark` runs the test, grades the results, and generates a set of analytical `measures` used to assess the `Candidate`.
* **Measure** - an numerical valude used to quantify the performance of a `Candidate` or the quality of its results. Derived from a the set of observed responses.
* **Metric** - a `Measure` designated to be used to officially assess a `Run`. While a `Run` may generate numerous `Measures` that are interesting for diagnostic purposes, it is only the `Metrics` that will be used to officially compare and assess systems.
* **Domain Data** - domain-specific data provided to `Candidate` before the start of a `Run`. Domain data might be provided via read-only or ephemeral mapped disk. 
* **White List** - a list of approved external service endpoints made available to a `Candidate`.
* **Training Run** - 
* **Characterizer** - a trusted system that generates aggregate characterizations of test data, used to gain insights leading to solutions. Characterizations are intended to be suitable for use outside of the laboratory environment.
* **Data Exploration** - interactive, organic exploration of raw data that leads to insights. Must be done inside of `Laboratory Environment`. Probably involves remotely accessing a `VM` in the `Laboratory Environment`. 

## Goals
1. **Securing Data-at-Rest** - we expect test data will be subject to a licensing terms that restrict its availability to a properly authenticated `Laboratory Environment`.
2. **Limiting ability of Candidates to "preview" data files** - we would like, as much as possible, to limit the opportunity for `Candidates` to preview the data files in a `Suite`. While a `Contestant` could submit a `Candidate` that records and exports its inputs via a whitelisted service, we would consider this unethical and a violation of the terms and conditions. Note that, in general, candidates cannot access network resources, tha are not on the approved whitelist.
3. **Reproducibility** - it should be possible, at any time, to reproduce the results of any historical `Run`. It should also be possible it redo a `Run` with an updated `Candidate` algorithm. 
4. **Transparency** - all aspects of `Candidate` configuration and `Suites` should be available for inspection. All historical `Run` data should be available for analysis. All `Measures` and `Metrics` should be available for inspection.
5. **Provide training cases** - we expect that many `Candidates` will leverage machine-learning techniques, and as such, will need access to training data.
6. **Provide consistent measures for each test run** - all `Measures` should be fully documented, and applied consistently across `Runs`. New `Measures` should be available on historic `Runs`.
7. **Ability to analyze and compare results** - would like to be able to see individual `Measures`, and `Trends` in `Measures` across time. Should be able to compare `Measures` from different `Candidates.`
8. **Ability to host other types of experiments in the future** - The system should not be hard-coded to work with one type of experiment. It should be possible to run completely different types of experiments in the future.
9. **Deployable to partner environments** - partners should be able to easily stand up their own `Laboratory Environments` and integrate them with their own CI processes.
10. **Interactive Data Exploration** - approved data scientists should be able to interactively explore the data with standard tools (e.g. notebooks, data frames, Spark etc).

## Design Facets

Each of the following design facets is followed by the list of goals it supports.
1. Users must accept contest rules as part of `Candidate` submission (1).
1. Users are authenticated (e.g. with Azure Active Directory) (1).
1. `Candidate` is a container that runs in the `Laboratory Environment` (1,2).
1. `Suite` assets are only available within the `Laboratory Environment` (1,2).
1. Benchmark runs the for-loop over the `Test Cases` (2,6)
1. All `Suites` are named, versioned, and archived (3,4).
1. All `Candidates` are named, versioned and archived (3,4).
1. All `Benchmarks` are named, versioned and archived (3,4).
1. All test `Run` logs are archived and indexed (4,7).
1. The `Einstein` `Laboratory` is also versioned.

----
## Terminology

* **Laboratory** - 
* **Benchmark** - 
* **Suite** - 
* **Measure** - 
* **Candidate** - 
* **Series** - 
* **Run** - 
* **Data** - 
* **(placeholder)** - 

## Test Run
* Decrypt candidate secrets
* Start candidate container with secrets volume
* Start benchmark container with test suite parameter

## Candidate
* Initialze
* Start up gRPC service
* Handle requests until instructed to exit
* Exit

## Benchmark

* Container runs
* Startup parameters
    * Candidate service endpoint
    * Candidate id
    * Suite id
* Wait for Candidate service to become responsive
* Run each test case in the suite
* Write out test run results
* Tell candidate to shut down
* Exit

