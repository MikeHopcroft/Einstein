# Einstein Design Overview

## Terminology
* **Contestant** - a person or other authenticated entity that submits `Candidates` and runs `Suites`.
* **Candidate** - an instance of a system under evaluation. While the system may be composed of multiple services and implemented in any technology, it is exposed to the `Benchmark` as a container that runs in the `Laboratory Environment`. In order to ensure reproducibility, it is important that the container's external dependencies maintain the same behavior over time. If container were to call an external service whose behavior could be modified by, say, training, the container could exhibit different behavior over time, and this would impact reproducibility. A best practice is to version all external dependencies and create a separate container for each version.
* **Suite** - a collection of `Test Cases` used to evaluate a `Candidate` with respect to a specific `Benchmark`. Note that the `Laboratory Environment` will typically contain multiple `Suites`, each intended for a different purpose.
* **Test Case** - a single stimulus-response pair. The `Benchmark` provides the stimulus to the `Candidate`, which generates a response. This response is compared with the expected response.
* **Run** - the results generated by a specific `Candidate`, processing cases from a specific `Suite`. All `Runs` are archived.
Ideally, each `Test Run` is reproducible, meaning that any future `Test Run` involving the same `Test Suite` and `Candidate System` should produce the same results. If the `Candidate System` depends on external systems or data that can change over time, it may be impossible to reproduce the results of an earlier `Test Run`.
* **Data** - `Test Case` inputs and expected outputs, used for model training and evaluation.
* **Laboratory** - the collection of services and data stores used to assess `Candidates`. The `Laboratory Environment` stores all data, code and assets needed to run `Suites` and retrieve and reproduce historical results.
* **Laboratory Environment** - a cloud environment in which the laboratory resides. Ensures proper data stewardship, primarily by restricting network access. Users must be approved and authenticated before gaining access to data and resources within the `Environment`. Network configurations prevent access to the outside world from within the `Environment`.
* **Operator/Administrator** - Person or entity running the `Laboratory Environment`.
* **Benchmark** - a system that runs `Tests` by supplying cases from a `Suite` to a `Candidate`. The `Benchmark` runs the test, grades the results, and generates a set of analytical `measures` used to assess the `Candidate`.
* **Measure** - an numerical valude used to quantify the performance of a `Candidate` or the quality of its results. Derived from a the set of observed responses.
* **Metric** - a `Measure` designated to be used to officially assess a `Run`. While a `Run` may generate numerous `Measures` that are interesting for diagnostic purposes, it is only the `Metrics` that will be used to officially compare and assess systems.
* **Domain Data** - static, domain-specific data provided to `Candidate` before the start of a `Run`. Domain data might be provided via read-only or ephemeral mapped disk. 
* **White List** - a list of approved external service endpoints made available to a `Candidate`.
* **Training Run** - 
* **Characterizer** - a trusted system that generates aggregate characterizations of test data, used to gain insights leading to solutions. Characterizations are intended to be suitable for use outside of the laboratory environment.
* **Data Exploration** - interactive, organic exploration of raw data that leads to insights. Must be done inside of `Laboratory Environment`. Probably involves remotely accessing a `VM` in the `Laboratory Environment`. 

## Goals
1. **Securing Data-at-Rest** - we expect test data will be subject to a licensing terms that restrict its availability to a properly authenticated `Laboratory Environment`.
2. **Limiting ability of Candidates to "preview" data files** - we would like, as much as possible, to limit the opportunity for `Candidates` to preview the data files in a `Suite`. While a `Contestant` could submit a `Candidate` that records and exports its inputs via a whitelisted service, we would consider this unethical and a violation of the terms and conditions. Note that, in general, candidates cannot access network resources, tha are not on the approved whitelist.
3. **Reproducibility** - it should be possible, at any time, to reproduce the results of any historical `Run`. It should also be possible it redo a `Run` with an updated `Candidate` algorithm. 
4. **Transparency** - all aspects of `Candidate` configuration and `Suites` should be available for inspection. All historical `Run` data should be available for analysis. All `Measures` and `Metrics` should be available for inspection.
5. **Provide training cases** - we expect that many `Candidates` will leverage machine-learning techniques, and as such, will need access to training data.
6. **Provide consistent measures for each test run** - all `Measures` should be fully documented, and applied consistently across `Runs`. New `Measures` should be available on historic `Runs`.
7. **Ability to analyze and compare results** - would like to be able to see individual `Measures`, and `Trends` in `Measures` across time. Should be able to compare `Measures` from different `Candidates.`
8. **Ability to host other types of experiments in the future** - The system should not be hard-coded to work with one type of experiment. It should be possible to run completely different types of experiments in the future.
9. **Deployable to partner environments** - partners should be able to easily stand up their own `Laboratory Environments` and integrate them with their own CI processes.
10. **Interactive Data Exploration** - approved data scientists should be able to interactively explore the data with standard tools (e.g. notebooks, data frames, Spark etc).

## Design Facets

Each of the following design facets is followed by the list of goals it supports.
1. Users must accept contest rules as part of `Candidate` submission (1).
1. Users are authenticated (e.g. with Azure Active Directory) (1).
1. `Candidate` is a container that runs in the `Laboratory Environment` (1,2).
1. `Suite` assets are only available within the `Laboratory Environment` (1,2).
1. Benchmark runs the for-loop over the `Test Cases` (2,6)
1. All `Suites` are named, versioned, and archived (3,4).
1. All `Candidates` are named, versioned and archived (3,4).
1. All `Benchmarks` are named, versioned and archived (3,4).
1. All test `Run` logs are archived and indexed (4,7).
1. The `Einstein` `Laboratory` is also versioned.

## Russian Dolls

We use a matryoshka doll anaology for feature delivery.
Our intention is to work inside-out, starting with the smallest doll. The goal is to deliver a minimum viable prototype for each doll before moving on to the next enclosing doll. The following is our current thinking of how functionality will be organized into "dolls".

1. There exists a secure, compliant `Environement` for hosting sensitive `Test Data`.
    * `Administrator` can grant and revoke access.
    * Networking is configured to prevent data egress.
    * `Data scientists` can use a VM to examine and characterize data, develop and train models, and run experiments.
    * Logging is configured to create audit trail for actions performed in the `Environment`.
    * Design has been threat-modelled and reviewed by security professionals.
1. A `Contestant` can develop a `Candidate` and have it evaluated by a `Benchmark`.
    * `Benchmark` exists
        * `Benchmark Protocol` is documented.
            * Logical description
            * API specification
            * Example of `Domain Data`
            * Example of `Test Suite`
        * `Benchmark` contains at least one `Test Suite`.
            * `Test Suite` is documented.
            * `Test Suite` is scientificially sound.
            * `Test Suite` provides at least one `Training Suite`.
        * `Benchmark` provides static `Domain Data`.
        * `Benchmark` implements at least one `Measure`.
    * There exists a mechanism for running a `Test Suite` associated with a `Benchmark` on a `Candidate`.
1. `Candidates` run in a secure, compliant `Benchmark Environment`.
    * Ability to submit a `Candidate` and initiate a `Test Run` for a specific `Test Suite` in a `Benchmark`.
    * Submission limited to authorized users.
    * Authorization requires human acknowledgement of terms and conditions.
    * Ability to view `Test Run Measures` outside of secure environment.
    * Ability to inspect diagnostic information outside of secure environment (e.g. view crash logss and examine input, observed answer, and expected answer)
1. `Operator` can demonstrate or produce evidence that the `Benchmark Enivronment` complies with applicable `Data Stewardship Policies`.
    * Work will depend on specifics of agreement signed with customer.
1. `Candidates` can gain access to certain, pre-approved services that are not hosted within the `Benchmark Environment`.
    * Mechanism to specify external services.
    * Process to vet and then approve or deny access to external services.
    * Mechanism to grant access to external services for the duration of a single `Test Run`.
    * Mechanism to isolate `Test Runs` so that a `Candidate` from one `Test Run` cannot access the external services of anotheer `Test Run`
1. `Test Run` configurations are archived
    * `Benchmarks` are versioned.
    * Archive `Benchmark` versions, including code, domain data, and test data.
    * `Candidates` are versions.
    * Archive `Candidate` versions.
    * `Test Runs` are versioned.
    * Archive `Test Run` configurations.
    * Provide some means to find and inspect archived data.
1. `Test Run` results are archived.
    * Log files
    * Run completion status (e.g. failed due to crash, run to completion)
    * Observed and computed `Measures`
1. `Test Run` results can compared.
    * Same `Candidate` across `Candidate Versions`
    * Same `Candidate` across `Test Suites`
    * Same `Candidate` across `Training Suites`
    * Different `Candidates` for same `Test Suite`
1. Multiple `Benchmarks`
    * `Benchmark` implementation documentation
        * Logical description
        * API specification
    * Process for submitting/installing a `Benchmark`
        * Limited to authorized users
1. Managing `Benchmark` Data
    * Adding `Test Case` data.
    * Assessing the impact of removing `Test Case` data.
    * Removing `Test Case` data.
    * Ability to access external data sources.
